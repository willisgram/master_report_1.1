%===================================== CHAP 7 =================================

\chapter{Results} \label{chapter_computational_study}

In this chapter, the results of running the solution framework outlined in Chapter \ref{chapter_solution_approach} on the 2017/2018 season of the FPL is presented. The framework is implemented with the numerical values for parameters computed in Chapter \ref{chapter_experimental_setup}. First, in Section \ref{sec:res_init_param}, the parameters in the mathematical model are initialized. Then, in Section \ref{sec:exact}, the mathematical model is run with realized points and the optimal ex-post decisions are presented. Furthermore, in Section \ref{sec:inexact}, the performance of the forecast-based optimization model for the FPL with the three different forecasting methods is presented. The model is also run with forecasts generated by the method suggested by \cite{Bonomo} in order to compare the performance to the performance when forecasts are generated by the Modified Average method. For all three methods, the model is first solved without the implementation of gamechips. Subsequently, the model is solved when gamechips are included and a discussion of the effect of the gamechips follows. Finally, in Section \ref{sec:res_risk_hand}, the risk handling constraints are included and the results are discussed.

\newpar

The mathematical model is written in the modelling language Mosel and implemented in FICO\textsuperscript {\textregistered} Xpress Optimization Suite 8.3, using a HP EliteDesk 800 G3 DM 65W computer with Intel\textsuperscript{\textregistered} Core\textsuperscript{\texttrademark} i7-7700 3.6 GHz processor and 32 GB RAM. The operating system in use is Windows 10 Education 64-bit. The input data is structured and pre-processed in the statistical programming language R. Further, the results obtained from the mathematical model include decisions on selected squad, starting line-up, substitution priority, number of penalized transfers, captain, vice-captain and whether a gamechip is used in a gameweek. These are written to CSV files, which are exported back to R to calculate how many points the team obtained. In the end, R is also used to make the data more presentable in form of tables and plots. 

\newpar

As mentioned in Section \ref{exp_setup_data}, the model is solved for the first 35 gameweeks of the 2017/2018 season. In general, it is important to note that for the forecast-based optimization model, no robust statements should be made due to the limited available data. However, insightful observations can be derived.

\section{Initialization of Parameters}\label{sec:res_init_param}   

Table \ref{tab:initializations_of_sets} and \ref{tab:initialization_of_parameters} display the initialization of parameters and sets used when the mathematical model is solved for both realized points and expected points. Notice that the realized points, $\rho$, is substituted with $\hat{\rho}$ when the model is run with expected points. Moreover, the penalty term, $R$, is set according to the values decided in Section \ref{Exp_setup_Player_Performance_Prediction}.

\begin{table}[H]
\centering

\begin{tabular}{@{}lll@{}}
\toprule
Set           &   &                                                               \\ \midrule
$\mathcal{T}$ & - & 35 gameweeks.                                             \\
$\mathcal{P}$ & - & 625 players.                                              \\
$\mathcal{C}$ & - & 20 teams.                                                 \\
$\mathcal{L}$ & - & \{1, 2, 3\}, where 1 is first priority. \\
$\mathcal{T}_{FH}$ & - & Gameweek 1 to 21 is in the first half of the season. \\
$\mathcal{T}_{SH}$ & - & Gameweek 22 to 35 is in the second half of the season. \\
\bottomrule
\end{tabular}
\caption{Initialization of sets.}
\label{tab:initializations_of_sets}
\end{table}

\begin{table}[H] 
\tabcolsep=0.11cm
\centering
\begin{tabular}{@{}lll@{}}
\toprule
Parameters                       &   &                                                                                                \\ \midrule
$\mathlarger{\rho_{pt}}$ & - & Realized points for a player $p$ in a gameweek $t$. \\
$\epsilon$                       & - & Set to 0.1.                                                                     \\
$\kappa_{1}, \kappa_{2}, \kappa_{3} $                     & - & Set to 0.01, 0.001 and 0.0001, respectively.                                               \\
$C_{pt}^{S}, C_{pt}^{B}$         & - & Value collected from FPL's homepage.  \\ 
$R$                              & - &  4 points deducted if number of free transfers is exceeded..       \\
$M^{K}$                          & - &  2 goalkeepers required in the selected squad.                                      \\
$M^{D}$                          & - &  5 defenders required in the selected squad.                         \\
$M^{M}$                          & - & 5 midfielders required in the selected squad.                                     \\
$M^{F}$                          & - & 3 forwards required in the selected squad.                                    \\
$M^{C}$                          & - & 3 players allowed to have from the same team.                                \\
$E$                              & - & 11 players required in the starting line-up.                              \\
$E^{K}$                          & - & 1 goalkeepers required in the starting line-up.                                       \\
$E^{D}$                          & - & 3 defenders required in the starting line-up.                  \\
$E^{M}$                          & - & 3 midfielders required in the starting line-up.                                 \\
$E^{F}$                          & - & 1 forward required in the starting line-up.                     \\
$B^{S}$                          & - & \pounds 100m as starting budget.                                                                              \\
$\beta$                          & - & Set to 1.                                                                                  \\          
$\bar{\alpha}$                   & - & Set to 14.                                                                      \\

$\phi$                           & - & 3 players are substitutes.                                                         \\
$\phi^{K}$                       & - & 1 keeper among the substitutes.                                                          \\
$\overline{Q}$                   & - & 2 free transfers possible to accumulate over gameweeks.                                              \\
$\underline{Q}$                  & - & 1 free transfer given every gameweek.                                      \\ \bottomrule
\end{tabular}
\caption{Initialization of parameters.}
\label{tab:initialization_of_parameters}
\end{table}


All the parameters specifically stated in the rules of FPL are set accordingly. These include number of players in different position in both selected squad and starting line-up, maximum players from same team, starting budget and the restrictions on free transfers. The parameter in the objective function for vice-captain are set to 0.1, while for the substitution priority, $\kappa_{1}, \kappa_{2}, \kappa_{3}$ are set to 0.01, 0.001 and 0.0001, respectively. To tighten the formulation as much as possible, the parameters $\beta$ and $\alpha$ are set so to the smallest, but sufficiently high, value. $\beta$ is used in constraints \eqref{eq:subst} and \eqref{eq:free_hit_subst} in Chapter \ref{chapter_model_formulation} which concern the substitution priority. Considering that the variable adopted are binary, setting the value to 1 is reasonable. $\alpha$ is used in constraints \eqref{eq:trans_flow_illegal_transfers}, and is set to highest possible value for number of penalized transfers. As 1 free transfer is given each gameweek and the selected squad consists of 15 players, it is not possible to have more than 14 penalized transfers. Hence, the parameter $\alpha$ is set accordingly. 


\section{Solution With Realized Points}\label{sec:exact}

Before the performance of the forecast-based optimization model for the FPLDP is examined, it is interesting to study the optimal solution when running the mathematical model with ex-post data. For one, the solution serves as a benchmark for the performance of the forecast-based optimization model for the FPLDP. Further, the optimal solution is compared to that of the top-performing manager, in order to see how close the best manager is to the optimal solution. This section is divided into two parts. First, a discussion of the results obtained by the mathematical model using realized points for the 2017/2018 season is presented. Here the solution is more thoroughly examined and the sensibility of the decisions are briefly discussed. Secondly, the solution is compared against the best performing human manager.

\newpar

As mentioned in Chapter \ref{mathematical_model}, a simplification is made by assuming that a player's sell price equals his value. However, the assumption is not expected to have a large impact on the optimal solution. For one, even if the model does not account for the concept of a sell-on fee, variations in a player's value is incorporated. Hence, when the model sells a player from the selected squad, it only obtains a slightly higher price than what it would do in reality. Furthermore, most players do not fluctuate with more than \pounds 0.2m during a season. Actually, only 3 players had a price change exceeding \pounds 1m during the 2017/2018 season. Also, among the players with the 10 largest price increases, the average change was 0.63, earning a profit of \pounds 0.3m.


\subsection{Problem Size}

Table \ref{tab:computational_statistics} displays the problem size of FPLDP with input data from the 2017/2018 season. The problem size is given before and after the function Presolve is run. This is a function integrated in Xpress Optimizer and its objectives are to reduce redundant variables, eliminate redundant constraints and remove linearly dependent constraints, which consequently reduces the complexity of the problem and improve the computing time. From the table it can be observed that the Presolve function is successful in eliminating variables, but the reduction of constraints is not that severe. A reason for this could be that most of the constraints are linearly independent, and that there is a limited presence of empty rows in the FPLDP. In addition, a large part of the variables $x_{pt}$, $x_{pt}^{freehit}$ and $y_{pt}$ are redundant, as many of them are defined even though only a few of them are used in the starting line-up and selected squad constraints. Thus, in order to reduce the solution time, an effort could have been put into reducing redundant variables in the procedure of implementation. However, since the model only has to be solved once, this is disregarded. 


\begin{table}[H]
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
                            & Rows(Constraints)    & Columns(Variables) 
                            \\ \midrule
Original Problem Statistics & 172 034 & 254 425   \\
Xpress Presolve Statistics  & 170 750 & 206 689   \\ 
\bottomrule
\end{tabular}
\caption{Problem size of the model run with realized points.}
\label{tab:computational_statistics}
\end{table}


\subsection{Results of Running the Model with Realized Points}

During testing, it was observable that nearly optimal solutions were found after 1000 seconds, hence the maximum running time was set to 86 400 seconds, or 1 day. This is illustrated in Figure \ref{fig:solutions_found_realized_points}. 

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.50]{fig/chapter_7/solution_found_edit_zoom_1.png}
    \caption{Graph of when the solutions are found in Mosel Xpress-MP.}
    \label{fig:solutions_found_realized_points}
\end{figure}


\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}llllllll@{}}
\toprule
Number of gameweeks                 & 5       & 10       & 15       & 20       & 25       & 30       & 35      \\ \midrule
%Rows, without presolve    & 24 043   & 48 708    & 73 373    & 98 038    & 122 704   & 147 369   & 172 034 \\
%Rows, presolve            & 22 787   & 47 448    & 72 108    & 96 768    & 121 429   & 146 090   & 170 750 \\
%Columns, without presolve & 35 275   & 71 800    & 108 325   & 144 850   & 181 375   & 217 900   & 254 425 \\
%Columns,, presolve        & 28 124   & 58 045    & 87 962    & 117 897   & 147 778   & 177 640   & 206 689 \\
Objective value           & 773.44 & 1380.95 & 2002.69 & 2660.42 & 3327.12 & 3956.76 & 4553.03 \\
Gap                       & -       & -        & -        & -        & -        & 0.27 \%  & 0.75 \% \\
Time to optimality        & 29      & 295      & 1 103     & 2 010     & 23 692    & N/A      & N/A     \\ \bottomrule
\end{tabular}%
}
\caption{Model run with different number of gameweeks.}
\label{tab:realized_points_diff_gameweeks}
\end{table}

In Table \ref{tab:realized_points_diff_gameweeks} it is observable that the optimal solution for 30 and 35 gameweeks was not found within maximum running time. For 35 gameweeks, the solution found only had a gap of 0.75 \%, which was regarded as sufficient to serve as a benchmark and compare against the top manager. It is important to note that the objective value given in Table \ref{tab:realized_points_diff_gameweeks} is from the model run, and hence not directly transferable to total points obtained in FPL. To test the model's scalability, the model was run for different numbers of gameweeks. The results are summarized in Table \ref{tab:realized_points_diff_gameweeks}, and illustrated in Figure \ref{fig:comp_time_diff_gameweeks}. It is notable that the solution time increases significantly when solved for more than 20 gameweeks.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.3]{fig/chapter_7/comp_time.png}
    \caption{Solution time when model is solved for different number of gameweeks.}
    \label{fig:comp_time_diff_gameweeks}   
\end{figure}

\subsection{Performance in Fantasy Premier League}

In the following, the optimal solution for 35 gameweeks is examined and discussed. Figure \ref{Figure_Realized_points} gives an overview of how many points the model obtained in each gameweek. The coloured dots represents gameweeks where the different gamechips were used. 

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{fig/chapter_7/perf_gc.png}
    \caption{Points per gameweek with realized points.}
\label{Figure_Realized_points}    
\end{figure}

As seen in Figure \ref{Figure_Realized_points}, the first and second Wildcards were used in gameweek 3 and 22, respectively. Thus, the model played the second Wildcard at the earliest stage possible in the second half of the season. Further, the Triple Captain was played in gameweek 31, which is reasonable as Liverpool's Mohammed Salah scored 4 goals and had 1 assist in this particular gameweek, yielding score of 29 points. This was the maximum number of points obtained in a gameweek by any Premier League player. The Free Hit was played in gameweek 26, a "normal" gameweek, while the Bench Boost was played in gameweek 34, a double gameweek. Thus, the only gamechip that was used consistent with the implementation in the forecast-based optimization model was the Bench Boost.

\newpar

In Figure \ref{Figure_Comparison}, the optimal solution is compared gameweek for gameweek against the weekly average points obtained among all managers and the points obtained by the top manager. It is also indicated when the top manager played the gamechips. The top manager never played a particular gamechip in the same gameweek as the optimal solution. The first Wildcard is played in gameweek 15 and the second Wildcard is played in gameweek 32. The Triple Captain is used in gameweek 22, a double gameweek. Moreover, the Free Hit is played in gameweek 35, a blank gameweek. Thus, the usage of the Triple Captain and Free Hit is consistent with the reasoning made for the forecast-based optimization model. Note that the use of the Bench Boost is not indicated, as it had not been played by the top manager until gameweek 35.

\begin{figure}[H]
\label{fig:Comparison}
    \centering
    \includegraphics[scale=0.5]{fig/chapter_7/perf_top_avg.png}
    \caption{Weekly average, top performer and solution with realized points for every gameweek.}
\label{Figure_Comparison}    
\end{figure}

 
From Figure \ref{Figure_Comparison} it is clear that the optimal solution substantially out-performs both the weekly average among managers and the top manager each gameweek. In Table \ref{Optimal_Human}\footnote{The data on rankings is retrieved from \url{https://fantasy.premierleague.com/}}, the performance of the top manager, as well as the performance required to reach different rankings among all managers, are compared against the optimal solution. It is clear that even the top manager only manages to achieve approximately 50\% of the points obtained by the optimal solution. One implication is that it is extremely hard to compete optimally in FPL, given the nature of the uncertainty in the problem. Moreover, the result can motivate the use of optimization based tools, as a potential for improvement certainly is seen when comparing the top manager's performance to the optimal solution.

\begin{table}[H]
\centering
\begin{tabular}{lcc}
\hline
                 & Mean   & \multicolumn{1}{l}{Percentage of optimal solution} \\
\hline                 
Optimal solution & 128.57 & 100.00 \%                                          \\
Top Manager           & 66.54  & 51.75 \%                                           \\
Top 5 \%         & 55.83  & 43.42 \%                                           \\
Top 10 \%        & 54.30  & 42.23 \%                                           \\
Top 20 \%        & 52.20  & 40.60 \%                                           \\
Top 30 \%        & 50.40  & 39.20 \%                                           \\
Top 40 \%        & 48.50  & 37.72 \%                                           \\
Top 50 \%        & 46.50  & 36.17 \%                                           \\
\hline
\end{tabular}
\caption{Comparing human managers to optimal solution.}
\label{Optimal_Human}
\end{table}

A curious observation with basis in Table \ref{Optimal_Human} is that the difference in mean between the top 50\% and the winner is only slightly above 20 points per round, while the difference between the optimal solution and the winner is above 60 points. Further, in mean, the difference between the winner and top 5\% is 10.71 points per gameweek. In comparison, the difference between finishing among the top 10\% and top 5\% is only an increased mean of 1.43 points per gameweek. A plausible explanation for the small difference is that a lot of managers select the same players. For instance, at some point, Mohammed Salah was selected by more than 63\% of the human managers. Thus, for the forecast-based optimization model, a slight increase in mean score can be enough to significantly improve the performance in terms of overall ranking. 


\section{Forecast-Based Solutions}\label{sec:inexact}


In this section, the results of the forecast based optimization model run with forecasts generated by the three different forecasting methods are presented. First, the performance when the gamechips are disregarded are presented and discussed. Then, the performance of each method when gamechips are included is presented. This serves two purposes. First, it is done in order to compare the methods against each other without the complicating inclusion of gamechips. Secondly, it allows a detailed examination of the effect of gamechips. Finally, the results are summarized and plausible explanations for the observations made are discussed. In order to evaluate the performance of the methods, their performances are compared to that of the optimal solution with realized points, the best overall manager and the weekly average among all managers. With respect to computational time, the model is solved in matter of seconds for all gameweeks.

\newpage

\subsection{Results Disregarding Gamechips}

In Figure \ref{fig:res_comp_dis_gamechips} the points obtained by the different forecasting methods are plotted for each gameweek. Table \ref{tab:res_dis_gamechips}, summarizes the results. Notice that for the Odds method results for 31 gameweeks, as data only is available for these gameweeks.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{fig/chapter_7/comparison_methods.png}
    \caption{Results of the different forecasting methods.}
\label{fig:res_comp_dis_gamechips}    
\end{figure}

Comparisons of the overall ranking are made in terms of mean points obtained. This is done because 31 gameweeks are considered for the Odds method and 35 gameweeks for the two other methods. From Table \ref{tab:res_dis_gamechips}, it is clear that the Modified Average and the Odds method outperform the Regression method. Moreover, it is clear the mean number of penalized transfers vary considerably between the methods. The difference between the Modified Average and Regression is particularly apparent. Furthermore, the Regression method has the highest volatility in terms of standard deviation. The standard deviations presented is empirical standard deviation calculated gameweek by gameweek.

\begin{comment}
Why? It is possible that you explained this in a former chapter. But even if you explained it you need to say it again. 

In any case, if you have results for 31 gws you should compare the three methods for only 31 gws. Otherwise some information is difficult to understand. For example: are 1650 points over 31 gws better or worse than 1765 over the entire season?
Top 13% after 31 weeks or after the entire season?
\end{comment}

\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Solution method     & Total points & Mean & St.Dev.  & Overall ranking & \makecell{Mean \\ penalized transfers}    \\
\hline
Modified Average    & 1916         & 54.74 & 15.85 & Top 8\%   & 0.23       \\
Regression          & 1765         & 50.43 & 20.73 & Top 30\%  & 1.80       \\
Odds (31 gameweeks) & 1650         & 53.23 & 13.94 & Top 13\%  & 0.81       \\
%Odds (28 gameweeks) & 1320         & 47.14 & & Top 47\%  & 0.39       \\
\hline
\end{tabular}%
}
\caption{Results disregarding gamechips.}
\label{tab:res_dis_gamechips}
\end{table}

 

\subsubsection{Modified Average}

From Figure \ref{fig:res_comp_dis_gamechips} it is evident that the Modified Average performs poorly in the first two gameweeks. A reasonable explanation is that the forecasts are based on the previous season. Also, several players are not taken into consideration in the first gameweeks, for instance due to promotions and international transfers. In fact, the Modified Average selects Gary Cahill and Cesc Fabregas, who both received a red card, in the first gameweek. With Cahill selected as captain, they deducted a total of -7 points.

\newpar

It is observable that the weekly results tend to stabilize from gameweek 3 to gameweek 18. This can stem from the fact that forecasts now include the realized points from previous gameweeks in the 2017/2018 season. Hence, the forecasting method is able to select the players who had a good start to the season. In addition, the transferred players from outside Premier League, including players from newly promoted teams, are now assigned expected points.

\newpar

From gameweek 19 to gameweek 35, the Modified Average improves its performance compared to that in the first part of the season. Again, a plausible explanation is that the forecasts are better, by for instance being able to capture which players are performing consistently well.


\subsubsection{Regression}


The Regression method displays the highest fluctuations in terms of standard deviation, and reaches the highest weekly realization. However, the method is outperformed by the two other in terms of mean. The weakest weekly performances are obtained in the beginning of the season. This is probably explained by the fact the each player is listed with low values for variables such as total goals, total assists, total saves etc., making it hard to distinguish the players from each other in the beginning of the season. In addition, also for the Regression method some players are not considered for the first gameweek due to transfers and promotions. The method achieves its highest weekly score in gameweek 20, earning 114 points. By comparison, the maximum number of points obtained by a manager this gameweek was 156 points. However, that manager played the Triple Captain. Adjusting for the gamechip use, the best manager earned 137 points, leaving a difference of only 23 points.

\subsubsection{Odds}

With a mean of 64 points in the 5 first gameweeks, the Odds method greatly outperforms the other methods in this interval. By comparison, the Modified Average earns a mean of 51 points and the Regression method earns a mean of 45 points. Furthermore, it has the overall lowest standard deviation. One explanation is that the forecasts of points are solely based on bookmakers predictions. Since bookmakers are professionals, it is not expected that the accuracy of their odds will change much over the season. Of course, bookmakers will adjust their calculations based on recent performance of players and teams. However, these adjustment will not be as critical as the adjustments for the two other solution methods. 

\newpar

The Odds method is beaten by the Modified Average method. An explanation is that the method has a limitation since forecasts only can be made one gameweek ahead. That is, the sub-horizon must be equal to 1. Note that the performance of the Odds method is not a direct indication of bookmakers' abilities to set probabilities as they are profit-seekers. 


\subsubsection{Comparing Modified Average with the solution suggested by \cite{Bonomo}}



The reader is reminded that the Modified Average method is inspired by the solution approach suggested for the Argentinian Fantasy League by \cite{Bonomo}. Therefore, it is interesting to compare the performance of the two different methods. From Figure \ref{fig:avg_vs_bon} and Table \ref{tab:bonomo_mofidified_average}, it is clear that the Modified Average outperforms the approach suggested by \cite{Bonomo}. As mentioned in Section \ref{Exp_setup_Player_Performance_Prediction}, \cite{Bonomo} set the track-length to 3 and the sub-horizon to 1. Note, however, that the Argentinian Fantasy League does not penalize transfers. Therefore, the penalty term is set to 4 as this is stated by the game rules of FPL. Thus, the basis for comparison is somewhat compromised and one should be careful not to draw too categorical statements about the two methods. Regardless, it is apparent that for the 2017/2018, the Modified Average performs constantly better, indicating the it is indeed an improvement of the method.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{fig/chapter_7/bon_gc_no_gc.png}
    \caption{Results of Modified Average and the solution approach suggested by \cite{Bonomo}.}
\label{fig:avg_vs_bon}    
\end{figure}

\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Solution method     & Total points & Mean & St.Dev.  & Overall ranking & \makecell{Mean \\ penalized transfers}     \\
\hline
Modified Average    & ??        & 47.03 & ??        & Top 47\%        & ??      \\
\cite{Bonomo}       & 1481         & 42.31 & 17.43 & Top 69\%  & 2.23       \\

\hline
\end{tabular}%
}
\caption{Performance of Modified Average and \cite{Bonomo}.}
\label{tab:bonomo_mofidified_average}
\end{table}


\subsection{Results Including Gamechips}

In Table \ref{tab:res_incl_gamechips}, a summary of the performance of each model when gamechips are included is presented. In general, by comparing the mean obtained with and without gamechips, it is apparent that the effect of gamechips is not consistent for the methods. The Regression method performs better, while both the Modified Average and Odds method perform worse.

\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Solution method     & Total points & Mean & Std.Dev.  & Overall ranking & \makecell{Mean \\ penalized transfers} \\
\hline
Modified Average    & 1881         & 53.74 & 16.62 & Top 12\%   &  0.23   \\
Regression          & 1901         & 54.31 & 20.32 & Top 10\%   &  1.20   \\
Odds (31 gameweeks) & 1557         & 50.23 & 15.17 & Top 30\%   & 0.48   \\
\hline
\end{tabular}%
}
\caption{Results including gamechips.}
\label{tab:res_incl_gamechips}
\end{table}



In Figure \ref{fig:res_comp_gamechips}, each method is plotted with and without gamechips. In the following, it is discussed how each gamechip have effected the solutions. Note that for each method, points obtained are similar for the first 6 gameweeks. As mentioned in Section \ref{exp_setup_gamechips} the model was allowed to use a Wildcard in gameweek 9, hence this decision was first considered in gameweek 7 due to the sub-horizon of 3 gameweeks. Therefore, the models make different decisions from that point on.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{fig/chapter_7/w_wo_gc_all.png}
    \caption{Performance of forecasting methods with gamechips.}
\label{fig:res_comp_gamechips}    
\end{figure}


To evaluate the impact of the first Wildcard,  comparison of performances are made from gameweek 7, as this is the first gameweek were decisions regarding the Wildcard are introduced. Moreover, only the effect until gameweek 19 is measured, as decisions regarding the Triple Captain are first introduced in this gameweek. As expected, Table \ref{tab:first_wildcard} shows that all methods perform a substantially higher number of transfers when the Wildcard is played. However, the effect of these transfers appear to be rather limited for the Modified Average and Odds method. The Regression, on the other hand, performs considerably better when the Wildcard is used.  

\newpar




\begin{table}[H]
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
                Method  & Mean GW 7-19  & Transfers GW 9 \\ \midrule
Modified Average w/gc   & 51.08 & 11             \\
Modified Average w/o gc & 57    & 1              \\
Regression w/gc         & 53.92 & 12             \\
Regression w/o gc       & 48.15 & 5              \\
Odds w/gc               & 46.62 & 11             \\
Odds w/o gc             & 49.31 & 1              \\ \bottomrule
\end{tabular}
\caption{Performance from gameweek 7 to 19.}
\label{tab:first_wildcard}
\end{table}


As can be seen in Table \ref{tab:performance_triple_captain}, all methods select either Harry Kane or Son Heung-min as Triple Captain in gameweek 22. First, it is worth noting that all players play for Tottenham, one of two teams that had a double gameweek. Furthermore, the Regression and Odds method had both Son and Kane in their starting line-ups. Thus, both the Regression and Odds had a higher forecast for Son than for Kane. A curious observation is that it was speculated in whether Kane would play both matches or not.\footnote{See article "Priceless Harry Kane a doubt for Tottenham's game with Swansea" at \newline \url{https://www.independent.co.uk/sport/football/premier-league/tottenham-spurs-transfer-news-harry-kane-no-price\\-illness-swansea-doubt-team-news-a8133251.html}} Such events can be caught by the Regression method by the variables number of transfers in and transfers out, and by the odds set by the decisions of bookmakers. For the Modified Average method, however, there is no way to pick up such signals. By not being able to incorporate any qualitative information such as line-up rumours, the Modified Average has a disadvantage when it comes to making the important decision as who to select for Triple Captain.

\begin{table}[H]
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
                 Method       & Triple Captain & Points \\ \midrule
Modified Average w/gc   & Harry Kane           & 9      \\
Modified Average w/o gc & Harry Kane           & 6      \\
Regression w/gc         & Son Heung-min            & 36     \\
Regression w/o gc       & Son Heung-min            & 24     \\
Odds w/gc               & Son Heung-min            & 36     \\
Odds w/o gc             & Son Heung-min            & 24     \\ \bottomrule
\end{tabular}
\caption{Performance of Triple Captain in gameweek 22.}
\label{tab:performance_triple_captain}
\end{table}

\newpar 

From Figure \ref{tab:performance_free_hit}, it is apparent that all methods perform better in the blank gameweek 31, the gameweek the Free Hit is used. Furthermore, by examining Table \ref{tab:performance_free_hit}, it is clear that difference is approximately 20 points for all methods. Furthermore, it is notable that only the Odds method makes a substantial amount of transfers ahead of the blank gameweek. This can be explained by the sub-horizon of 1, as the other methods can plan 3 matches ahead. Moreover, for the Modified Average, there are large differences in the number of players playing with and without gamechips compared to the other methods. This can be due to the fact that the parameter tuning was done without taking the Free Hit into consideration. Hence, a penalty term of 16 is perhaps not optimal when the Free Hit is taken into consideration. It is worth noting that all methods obtain a relatively high score in gameweek 31. This is mostly explained by the fact that Mohammed Salah earned 29 points for the particular gameweek. As all methods had him as captain, they earned a score of 58 points on him alone.

\begin{table}[H]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
Method                  & Points GW31 & Transfers  & Players playing \\ \midrule
Modified Average w/gc   & 92          & 14         & 11              \\
Modified Average w/o gc & 74          & 0  & 5               \\
Regression w/gc         & 104         & 13         & 11              \\
Regression w/o gc       & 87          & 2  & 9               \\
Odds w/gc               & 97          & 15         & 11              \\
Odds w/o gc             & 61          & 10  & 11              \\ \bottomrule
\end{tabular}
\caption{Performance of Free Hit in gameweek 31.}
\label{tab:performance_free_hit}
\end{table}


\newpar

As the second Wildcard and Bench Boost are played in cooperation, they are discussed together. The second Wildcard was played in gameweek 33, preparing for the double gameweek 34. Data for these gameweeks are only available for the Modified Average and Regression method, so the discussion is limited to these methods. Since the Bench Boost is played in gameweek 34, it is considered by the model from gameweek 32. Gameweek 31 is the gameweek when the Free Hit is played. Therefore, only gameweek 32-34 is  considered when evaluating the effect of the second Wildcard and Bench Boost. Table \ref{tab:performance_wildcard_and_bench_boost} shows that in terms of mean points obtained, the effect of the Wildcard and Bench Boost for gameweek 32-34 is positive only for the Regression method. In particular, the Modified Average performs weaker in the gameweek the Wildcard is played, while the Regression performs better. Furthermore, both methods improve in the gameweek the Bench Boost is played. However, as 15 players are awarded points, this is expected.


\begin{table}[H]
\centering
\begin{tabular}{@{}lcccc@{}}
\toprule
Method                  & GW 32 & GW 33 (W) & GW 34 (BB) & Mean \\ \midrule
Modified Average w/gc   & 37    & 31        & 88         & 52   \\
Modified Average w/o gc & 48    & 42        & 81         & 57   \\
Regression w/gc         & 42    & 32        & 91         & 55   \\
Regression w/o gc       & 48    & 23        & 61         & 44   \\ \bottomrule
\end{tabular}
\caption{Performance of Wildcard and Bench Boost in GW32-34.}
\label{tab:performance_wildcard_and_bench_boost}
\end{table}






\subsection{Summary}

\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
\def\arraystretch{1.5}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Solution method            & Total points              & Mean                       & Std.Dev.                    & Overall ranking               & \makecell{Mean \\ penalized transfers} \\ \hline
Modified Average w/ gc     & 1881                      & 53.74                      & 16.62                      & Top 12\%                      & 0.23                                                                                \\ \hline
Modified Average w/o gc    & 1916                      & 54.74                      & 15.85                      & Top 8\%                       & 0.23                                                          \\ \hline
Regression w/ gc           & 1901                      & 54.31                      & 20.32                      & Top 10\%                      & 1.20                                                                                \\ \hline
Regression w/o gc          & 1765                      & 50.43                      & 20.73                      & Top 30\%                      & 1.80                                                           \\ \hline
Odds (31 gameweeks) w/ gc  & 1557                      & 50.23                      & 15.17                      & Top 30\%                      & 0.48                                                                                \\ \hline
Odds (31 gameweeks) w/o gc & 1650                      & 53.23                      & 13.94                      & Top 13\%                      & 0.81                                                           \\ \hline
Realized points            & 4500                         & 128.57                          & 16.50                          & N/A                             & 4.23  
                             \\ \hline
Weekly Average             & 1699                      & 48.53                      & 7.99                       & Top 40\%                      & -                                                                                   \\ \hline
Top Manager                & 2329                      & 66.54                      & 18.28                      & Top 1 â€°                       & 0.11                                                                                \\ \hline
\end{tabular}%
}
\caption{Summary of the results.}
\label{tab:summary_of_results}
\end{table}



In Table \ref{tab:summary_of_results}, the results of all different methods with and without gamechips are summarized. Furthermore, the solution with realized points is given as a benchmark. However, the model with realized points significantly beats the forecast-based optimization model. Therefore, as a more realistic benchmark for top performance, the performance of the top manager is included. The best manager only manages to achieve approximately 50\% of what the solution with realized points does. This speaks in volumes of the overall uncertainty in the FPL. Furthermore, it motivates the potential of data-driven models. As a benchmark of average performance, the FPL's weekly average is used.

\newpar

As the season consists of 38 gameweeks, comparing overall performance is somewhat problematic when only gameweek 1-35 is considered. For instance, the top-manager still has a gamechips left. This is also an issue when comparing the Odds method with the two other methods. Nevertheless, the results should be indicative as more than 5.9 million people play the game and plenty will already have played all the gamechips. Furthermore, it is  stressed that as testing is only performed on one season, overly categorical statements should not be made. Nonetheless, with either the Modified Average method without gamechips, or the Regression based approach with gamechips, the forecast-based optimization model appears to have the potential to reach the top 10\% of managers in the FPL. Furthermore, a position in the top 30\% is achieved by all methods. Thus, beating the average weekly performance certainly appears to be obtainable. On the other hand, all models are far behind the top manager. Without further improvement, winning the FPL with the forecast-based optimization model appears to be unobtainable. 

\newpar

The Odds method performs best over the first five gameweeks. As the Odds method is based on bookmakers odds, it is not as heavily influenced by past season's performance as the Modified Average and Regression method. In fact, from the previous season's Dream team, i.e. the 11 players that collected most points, only one player had a score above two points in the first gameweek. In addition, the Odds method does not exclude players due to promotion or transfers to Premier League ahead of the season. The observation is substantiated the fact that in gameweek 5, when entire track-length is first founded in the 2017/2018 season, the Modified Average method starts to outperform the Odds method. Hence, the Odds method appears to be the best suited forecasting method in the earliest gameweeks of the season, but not necessarily in later gameweeks.

%Furthermore, the Modified Average method outperforms the other methods from gameweek 5-10. One possible reason is that now, the whole track-length is founded in the 2017/2018 season.

\newpar

As evident from Table \ref{tab:summary_of_results}, the number penalized transfers is quite different between the methods. In particular, it is high for the Regression method. It can be argued that the higher penalty term incorporated for Modified Average is only a compensation for inaccurate forecasts. Thus, it appears as if the Regression method in particular also could benefit from a parameter tuning approach with respect to the penalty term $R$. However, this was constrained by the limited availability of data. Hence, it is arguable that it is unfair to compare the performance of the Regression method with the Modified Average method. When more data becomes available, a parameter tuning conducted for the Regression method is an exciting topic for future research.

\newpar 

In general, the methods exhibit different variability. Based on the standard deviations presented in Table \ref{tab:summary_of_results}, the Regression method is most volatile, while the Odds method is the least. One explanation is that the Regression method does not account for recent performance in as a direct manner as the Modified Average method does by averaging points in the last rounds. Moreover, odds are set by the standard procedures of professional bookmakers, while the other methods are solely based on data and will to a larger degree incorporate extreme and unexpected past events. Moreover, the performance for all methods appear to fluctuates more in the second half of the season. One sensible reason is the emergence of blank and double gameweeks. 

\newpar

The effect of gamechips is not consistent for all methods. The Triple Captain and Free Hit appear to boost performance for all methods. The Second Wildcard in combination with Bench Boost appear to have a favorable effect on the Regression model, but not for the Modified Average. The same goes for the first Wildcard. This can be due to several reasons. First, for the Regression method in Section \ref{exp_setup_reg}, it is found that time-series effect of points is rather limited. Hence, for the Regression, only variables such as total amount of goals, assists, saves etc. are considered. When playing the Wildcard, the entire selected squad can be changed, and the new selected squad is carried forwards. If the Wildcard is played and forecasts are based on the Regression method, the performances of players over all the previous gameweeks are considered. For the Modified Average, on the other hand, the forecasts are only based on the last 5 matches. When considering to change the entire selected squad, i.e. playing the Wildcard, taking  all previous gameweeks into consideration seems to be the preferable approach. Secondly, the parameter tuning is done without gamechips in mind. That is, neither track length nor sub-horizon are optimized with respect to the Wildcard. As all gamechips are meant to provide opportunities that benefit managers, it is counter intuitive that the results are worse when they are included. In the end, the varying results of the model run with gamechips is due to the implementation of the gamechips. In fact, if the gamechips are used after the model is run without first taking them into consideration, they can be guaranteed not to have a negative influence on performance. The Wildcards and Free Hit can be used only in order to "pay" for penalized transfers after decisions are made, while the Triple Captain and Bench Boost can be played after the captain and the selected squad are decided. Thus, the gamechips will only have a positive or zero effect. As seen, however, this is not guaranteed when implementing the gamechips with the strategies suggested in this thesis. A possible explanation of the bad performance when gamechips are included is that an implementation solely based on strategy is not ideal for an optimization model. Since the use of gamechips are fixed to certain gameweeks, the solution space is decreased and the solutions found are sub-optimal. The implementation of gamechips is regarded as one of the areas with largest potential for improvement in the forecast-based optimization model.


\section{Risk Handling} \label{sec:res_risk_hand}

\begin{comment}
In this section the effect of adding risk handling to the mathematical model is examined. That is, constraints \ref{eq:variance} to \ref{eq:variance_p_p_dash_beta} are included in the mathematical model formulated in Section \ref{mathematical_model}. Furthermore, we run the model with changing $\sigma_{0}^2 \in [\underline{\sigma^2}, \overline{\sigma^2}]$. Forecasts from the the Modified Average method are used when running the model. As the Regression method exhibits most variance, this method is best suited for risk handling. However, as outlined in Section \ref{exp_setup_Value_Variance}, the variance of each player is estimated as the empirical variance. The task of estimating variance is more complex for the regression model, as the explanatory variables are assumed to explain some of the variance. Therefore, forecasts from the Modified Average model is used. As the performance is measured in the unit points, the thresholds are given as standard deviations rather than variances. Furthermore, we emphasize that by threshold we mean a maximum standard deviation threshold sat before a model is run. When standard deviations are discusses, they refer to the realized standard deviation for a model run for an entire season. 
\end{comment}

In this section, the effect of risk handling in the forecast-based optimization model is examined. That is, constraints \ref{eq:variance} to \ref{eq:variance_p_p_dash_beta} are included. The model is run with changing thresholds $\sigma_{0}^2 \in [\underline{\sigma^2}, \overline{\sigma^2}]$. However, since the performance is measured in the unit points, the thresholds are from now on denoted as standard deviations rather than variances. Further, it is emphasized that a \textit{threshold} refers to a value chosen before a model is run. \textit{Realized standard deviation}, on the other hand, refers to an empirical value computed after the model is run for all gameweeks. Forecasts from the Modified Average method are used and gamechips are included. Including the risk handling constraints increase the computational time significantly. For the lowest threshold the model is solved in approximately 20 minutes for all gameweeks.

\newpar

\begin{comment}
The aim is to discuss the effect of risk handling on the solution, and examine the performance to see if there are any similarities with the variance effect in portfolio optimization. It is important to emphasize that one can not take any general conclusions on the impact, as this is bases on realization of points in one season. Nevertheless, a season includes 35 data points on 625 players, and consequently useful insight is obtainable from such analysis.  
\end{comment}


\newpar

In Sections \ref{Other_Relevant_Research} and  \ref{Ch.5_Variance_tradeoff}, it is pointed out that the FPLDP resembles a classic portfolio optimization problem. Therefore, it is interesting to investigate if there exists a similar expected return/risk trade-off in the FPLDP as it does in portfolio optimization. Accordingly, the effect varying thresholds has on expected points for the first sub-problem is studied. The mathematical model maximizes a team's expected points. Hence, one would assume that when the threshold is decreased, the expected value of the team also decreases. This would coincide with the \textit{efficient frontier} found in portfolio optimization. The efficient frontier consists of the optimal set of portfolios that gives the highest expected points for a certain level of risk. In Figure \ref{fig:threshold_GW1}, the expected points, i.e. the objective value, obtained by the optimization model for the first sub-problem is plotted against different thresholds. The curve exhibits a similar behavior as the efficient frontier, but is not smooth. This is because in the FPLDP one can either include a player, or not. That is, the decision is binary. In other words, opposed to the classic portfolio optimization problem, one can not hold a fraction of an asset in the FPLDP. 
\newpar

 
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{fig/chapter_7/GW1_var.png}
    \caption{Threshold plotted against objective value in the mathematical model for the first sub-problem.}
\label{fig:threshold_GW1}    
\end{figure}%


In Figure \ref{fig:performance_varying_threshold}, the mean points obtained in the 2017/2018 season when the model is run with different threshold is plotted. From Figure \ref{fig:performance_varying_threshold}, it is observable that the plot deviates from that of the efficient frontier. However, some similarities can be found. Higher thresholds are for instance associated with higher mean scores, and lower thresholds are associated with lower means. Note, however, that a portfolio's return does not necessarily follow the efficient frontier when an analysis of realized returns is performed. When the constraints are non-binding, the mean stabilizes at 54.34 points. Intuitively, the mean should have been that same as when the model is run without the constraints at 53.74. However, after examining the objective values, it is clear that there are different solutions with the same expected value for the first sub-problem. Thus, there is no guarantee that the solver selects the same solution. A possible way around is a different implementation of variance constraints. A \textit{trade-off approach} \citep{Speranza} is an interesting alternative, where instead of bounding the variance it is rather taken into consideration by moving it to the objective function.


\begin{comment}
In Figure \ref{fig:performance_varying_threshold}, the mean points of the 2017/2018 season is plotted for different thresholds. Due to computational difficulties in the software, Mosel Xpress-MP, the model was stopped when a solution within 2 \% optimality gap was found. From Figure \ref{fig:performance_varying_threshold}, it is observable that the plot deviates from that of the efficient frontier, but some similarities can be found. Higher thresholds are for instance associated with higher mean scores, and lower thresholds are associated with lower mean scores. Note, however, that a portfolio's return does not necessarily follow the efficient frontier when an analysis of realized returns is performed.

\end{comment}

\newpar

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{fig/chapter_7/var.png}
    \caption{Results of the performance when varying threshold.}
\label{fig:performance_varying_threshold}    
\end{figure}%


As seen in Figure \ref{fig:performance_varying_threshold}, the best mean score is not reached for an unlimited threshold, but for a threshold of 55 points. For this point, the method would have achieved mean of 57.54 points, earning a spot among the top 1.5\% of all managers. In general, the solutions outperform the solution with an unlimited threshold in the area with a threshold between 55-70 points. In Figure \ref{fig:performance_different_threshold}, the solutions with threshold 55 and 7.5 are plotted against the solution without a threshold. It is noticeable that the variance appears to be higher than for the unlimited case when the threshold is 55, but not 7.5. In Table \ref{tab_high_low_thresholds}, the mean and realized standard deviation for thresholds from 7.5-12.5 and from 55-65 are shown. Based on the table, it is clear that the goal of achieving lower realized standard deviation in the solution is reached with lower thresholds. Furthermore, in these cases, the mean is reduced, analogous to the case of portfolio optimization. Observe, however, that when the thresholds are between 7.5-12.5, they are unable to keep the realized standard deviations below the thresholds. The realized standard deviation for the thresholds in the region 55-65 exceeds the realized standard deviation of the unlimited case. For threshold between 55-65, however, the realized standard deviation is well below the threshold. Yet, as the threshold is approximately as high as the mean points obtained, it is counter intuitive that such a threshold should be binding. Thus, the positive effect in mean is probably related to the nature of the problem, not to the variance reduction per se. That is, having a factor decide how many players of the same and opposing teams to include in the selected squad can be a beneficial strategy in terms of improving mean score, even if it does not necessarily reduce the realized standard deviation of points obtained.




\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{fig/chapter_7/var_threshold.png}
    \caption{Plot of the results when $\sigma_0^{2} = 55^2$, $\sigma_0^{2} = 7.5^2$ and without variance constraints.}
\label{fig:performance_different_threshold}    
\end{figure}%
\newpar



\begin{table}[H]
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
Threshold & Mean & Realized St.Dev. \\ \midrule
7.5       & 51.77 & 15.7   \\
10        & 50.77 & 14.6   \\        %Mean = 51.9
12.5      & 51.49 & 15.9   \\
Unlimited & 53.74 & 16.8   \\
55        & 57.54 & 20.8   \\
60        & 55.71 & 18.1   \\
65        & 55.51 & 18.8   \\ \bottomrule 
\end{tabular}
\caption{Results for the high and low thresholds.}
\label{tab_high_low_thresholds}
\end{table}

\newpar

To summarize, a threshold between 55-70 points appears to have a positive effect on performance in terms of mean, but not in terms of reducing realized standard deviation. Furthermore, a threshold of 7.5-15 points appears to be able to lower realized standard deviation at the cost of mean obtained. An interval where both the realized standard deviation is decreased, and the performance is increased, is not found. Again, it is stressed that all results are based on data of only two seasons and are indicative at best.







\begin{comment}
In the end, it is interesting to investigate how different the teams actually are with the introduction of variance constraints. Do the model choose players which are stable and always achieves a certain amount of points? And, do the model choose players which plays for opposing teams? Or the model always try to pick players which do not play against each other? In general, do the behaviour of the model imitate the analogy of diversification in portfolio management? If so, this is a great finding. 
\end{comment}







